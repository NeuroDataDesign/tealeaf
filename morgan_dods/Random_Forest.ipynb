{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Random Forest Implementation Converted to Pandas\n",
    "\n",
    "https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
    "I took the random forest implementation from the link above, and converted the data format from lists of lists to pandas DataFrames and Series to further understand how the random forest algorithm works. There is a bug in my current implementation that causes all accuracy scores to be 0, but I still feel I accomplished the object of this exercise: to deepen my understanding of coding a RF algorithm so I can follow the SPORF code and begin writing MORF in cython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(data, n_folds):\n",
    "    data_split = []\n",
    "    data_copy = list(data)\n",
    "    fold_size = int(len(dataset) / n_folds) # find # of samples per fold\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size: \n",
    "            # add samples to fold list until size reached\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold) # then add to list of folds\n",
    "    return dataset_split # return list of fold lists\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    # for each idx in y_true and y_pred, add to num correct\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0 # calculate accuracy\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    # split data into n folds saved in folds (a list of folds (each with samples))\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    \n",
    "    scores = list()\n",
    "    for fold in folds: # for every fold\n",
    "        train_set = list(folds) #copy whole folds list of lists\n",
    "        train_set.remove(fold) # remove fold of interest\n",
    "        train_set = sum(train_set, []) # add an empty list to the end\n",
    "        test_set = list() \n",
    "        # for each sample in the fold\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy) # copy sample and append it to test set\n",
    "            row_copy[-1] = None # remove label from sample\n",
    "        predicted = algorithm(train_set, test_set, *args) #predict based on clf w/ train and test set\n",
    "        actual = [row[-1] for row in fold] # generate y_true\n",
    "        accuracy = accuracy_metric(actual, predicted) # compute accuracy score\n",
    "        scores.append(accuracy) # add to a list of scores for every split\n",
    "    return scores # return scores for each split\n",
    " \n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset: # for each sample \n",
    "        if row[index] < value: # split samples based on a single feature's value\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups])) \n",
    "    # sum list of lengths of groups (2 way split based on one feature value)\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size #count all appearances of class val in group \n",
    "                                                                    # divide by length of group\n",
    "            score += p * p # add square of this to score\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances) \n",
    "    return gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    # obtain unique class labels\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    # best index, value, score, and groups\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    #empty list of features\n",
    "    features = list()\n",
    "    # select random feature indices until n_feats reached\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    # for each feature idx and sample\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            # groups is a 2 way split based on the selected index and the value of that feature in that sample\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values) # calculate gini score for each group (low is good)\n",
    "            if gini < b_score: # get the best split over all selected features and samples\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group] # list of y_true for L and R groups\n",
    "    return max(set(outcomes), key=outcomes.count) # which class appears most in this group?\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups'] # copy L and R groups for node\n",
    "    del(node['groups']) # delete groups from the node\n",
    "    # check for a no split\n",
    "    if not left or not right: # what is not a list?? no split?\n",
    "        node['left'] = node['right'] = to_terminal(left + right) #means this node is a leaf node\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth: # if at max depth make left and right terminal nodes\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size: # check if enough samples in left node to split\n",
    "        node['left'] = to_terminal(left) # if not, this is a leaf node\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features) # otherwise, get the best split again\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1) # run this function again\n",
    "    # process right child\n",
    "    if len(right) <= min_size: # same as left\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    " \n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features) # get split with lowest gini score for training set\n",
    "    split(root, max_depth, min_size, n_features, 1) # recursively split until no split, max_depth, or min_size reached\n",
    "    return root # split alters the tree, which is a dictionary of dictionaries\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    # node['left'] and node['right'] is either a dict or an int depending on whether terminal\n",
    "    if row[node['index']] < node['value']: # determine whether sample falls on left or right side of this node\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row) # continue down the tree unless a leaf node is passed\n",
    "        else:\n",
    "            return node['left'] # if a leaf node is reached return the class value\n",
    "    else: # same for right side\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "        \n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list() # empty sample\n",
    "    n_sample = round(len(dataset) * ratio) # number of samples based on ratio of dataset to sample\n",
    "    while len(sample) < n_sample: # get n samples by randomly choosing indices\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample # list of samples (not indices)\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees] # list of predictions for a sample for all trees\n",
    "    return max(set(predictions), key=predictions.count) # most common prediction\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list() # empty list of trees\n",
    "    for i in range(n_trees): # generate n tree dicts and put them into tree list\n",
    "        sample = subsample(train, sample_size) # train on random samples from training set\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features) #build tree using max depth, min size, and n features\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test] #generate list of predictions for test set\n",
    "    return(predictions) # return this list of test predictions (Why isn't this a separate function??)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Iris and Sonar dataset\n",
    "# need to make pandas/numpy version of RF\n",
    "\n",
    "\n",
    "def cross_validation_split(data, n_folds):\n",
    "    cols = list(data.columns)\n",
    "    fold_size = int(len(data) / n_folds) # find # of samples per fold\n",
    "    data_split = []\n",
    "    data_copy = data.copy()\n",
    "    for i in range(n_folds):\n",
    "        fold = pd.DataFrame(columns = cols)\n",
    "        while len(fold) < fold_size: \n",
    "            # add samples to fold list until size reached\n",
    "            index = random.randrange(len(data_copy.index))\n",
    "            fold = fold.append(data_copy.loc[data_copy.index[index]])\n",
    "            data_copy = data_copy.drop(data_copy.index[index])\n",
    "        # then add to list of folds\n",
    "        data_split.append(fold)\n",
    "    return data_split # return list of fold lists\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    # for each idx in y_true and y_pred, add to num correct\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0 # calculate accuracy\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(data, algorithm, n_folds, *args):\n",
    "    # split data into n folds saved in folds (a list of folds (each with samples))\n",
    "    folds = cross_validation_split(data, n_folds)\n",
    "    scores = []\n",
    "    for fold_idx in range(len(folds)): # for every fold\n",
    "        train_set_ = list(folds) #copy whole folds list of lists\n",
    "        del train_set_[fold_idx] # remove fold of interest\n",
    "        #train_set = sum(train_set, []) # add an empty list to the end\n",
    "        cols = list(data.columns)\n",
    "        train_set = pd.DataFrame(columns = cols)\n",
    "        for i in train_set_:\n",
    "            train_set = train_set.append(i)\n",
    "        test_set = pd.DataFrame(columns = cols)\n",
    "        # for each sample in the fold\n",
    "        for index, row in folds[fold_idx].iterrows():\n",
    "            row_copy = row.copy()\n",
    "            row_copy.pop(cols[-1])\n",
    "            test_set = test_set.append(row_copy) # copy sample and append it to test set\n",
    "            #row_copy[-1] = None # remove label from sample\n",
    "        predicted = algorithm(train_set.reset_index(), test_set.reset_index(), *args) #predict based on clf w/ train and test set\n",
    "        actual = [row[cols[-1]] for index, row in folds[fold_idx].iterrows()] # generate y_true\n",
    "        accuracy = accuracy_metric(actual, predicted) # compute accuracy score\n",
    "        scores.append(accuracy) # add to a list of scores for every split\n",
    "    return scores # return scores for each split\n",
    " \n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(feat, value, data):\n",
    "    left = data[data[feat] < value]\n",
    "    right = data[data[feat] >= value]\n",
    "    #for row in dataset: # for each sample \n",
    "    #    if row[index] < value: # split samples based on a single feature's value\n",
    "    #        left.append(row)\n",
    "    #    else:\n",
    "    #        right.append(row)\n",
    "    return left.reindex(), right.reindex()\n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes, label_name):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups])) \n",
    "    # sum list of lengths of groups (2 way split based on one feature value)\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            #p = [row[-1] for row in group].count(class_val) / size #count all appearances of class val in group \n",
    "            p = len(group[group[label_name] == class_val]) / size                              # divide by length of group\n",
    "            score += p * p # add square of this to score\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances) \n",
    "    return gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "def get_split(data, n_feats):\n",
    "    cols = list(data.columns)\n",
    "    # obtain unique class labels\n",
    "    class_values = np.unique(data[cols[-1]])\n",
    "    # best index, value, score, and groups\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    #empty list of features\n",
    "    features = []\n",
    "    columns = list(data.columns)\n",
    "    # select random feature indices until n_feats reached\n",
    "    while len(features) < n_features:\n",
    "        index = random.randrange(data.shape[1]-1)\n",
    "        if columns[index] not in features:\n",
    "            features.append(columns[index])\n",
    "    # for each feature idx and sample\n",
    "    for feat in features:\n",
    "        for index, row in data.iterrows():\n",
    "            # groups is a 2 way split based on the selected index and the value of that feature in that sample\n",
    "            groups = test_split(feat, row[feat], data)\n",
    "            gini = gini_index(groups, class_values, cols[-1]) # calculate gini score for each group (low is good)\n",
    "            if gini < b_score: # get the best split over all selected features and samples\n",
    "                b_feat, b_value, b_score, b_groups = feat, row[feat], gini, groups\n",
    "    return {'index':b_feat, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    cols = list(group.columns)\n",
    "    #outcomes = row['species'].value_counts().max() # list of y_true for L and R groups\n",
    "    return group[cols[-1]].value_counts().max() # which class appears most in this group?\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups'] # copy L and R groups for node\n",
    "    del(node['groups']) # delete groups from the node\n",
    "    # check for a no split\n",
    "    if len(left) < 1 or len(right) < 1: # what is not a list?? no split?\n",
    "        node['left'] = node['right'] = to_terminal(left.append(right)) #means this node is a leaf node\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth: # if at max depth make left and right terminal nodes\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size: # check if enough samples in left node to split\n",
    "        node['left'] = to_terminal(left) # if not, this is a leaf node\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features) # otherwise, get the best split again\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1) # run this function again\n",
    "    # process right child\n",
    "    if len(right) <= min_size: # same as left\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    " \n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features) # get split with lowest gini score for training set\n",
    "    split(root, max_depth, min_size, n_features, 1) # recursively split until no split, max_depth, or min_size reached\n",
    "    return root # split alters the tree, which is a dictionary of dictionaries\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    # node['left'] and node['right'] is either a dict or an int depending on whether terminal\n",
    "    if row[node['index']] < node['value']: # determine whether sample falls on left or right side of this node\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row) # continue down the tree unless a leaf node is passed\n",
    "        else:\n",
    "            return node['left'] # if a leaf node is reached return the class value\n",
    "    else: # same for right side\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "        \n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(data, ratio):\n",
    "    cols = list(data.columns)\n",
    "    sample = pd.DataFrame(columns = cols) # empty sample\n",
    "    n_sample = round(len(data) * ratio) # number of samples based on ratio of dataset to sample\n",
    "    while len(sample) < n_sample: # get n samples by randomly choosing indices\n",
    "        index = random.randrange(len(data.index))\n",
    "        sample = sample.append(data.iloc[data.index[index]])\n",
    "    return sample # list of samples (not indices)\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees] # list of predictions for a sample for all trees\n",
    "    return max(set(predictions), key=predictions.count) # most common prediction\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = [] # empty list of trees\n",
    "    for i in range(n_trees): # generate n tree dicts and put them into tree list\n",
    "        sample = subsample(train, sample_size) # train on random samples from training set\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features) #build tree using max depth, min size, and n features\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for index, row in test.iterrows()] #generate list of predictions for test set\n",
    "    return(predictions) # return this list of test predictions (Why isn't this a separate function??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morgs/miniconda3/envs/tealeaf/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "splits = cross_validation_split(iris, 5)\n",
    "\n",
    "count = 0\n",
    "for i in iris['species'].unique():\n",
    "    iris['species'][iris['species'] == i] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(math.sqrt(iris.shape[1]-1))\n",
    "\n",
    "evaluate_algorithm(iris, random_forest, n_folds, max_depth, min_size, sample_size, 10, n_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Trees: %d' % n_trees)\n",
    "    #print('Scores: %s' % scores)\n",
    "    #print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    6.1\n",
       "sepal_width       3\n",
       "petal_length    4.9\n",
       "petal_width     1.8\n",
       "species           2\n",
       "Name: 127, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.iloc[127, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar = pd.read_csv('sonar_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_algorithm(sonar, random_forest, n_folds, max_depth, min_size, sample_size, 5, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "0         0.0200       0.0371       0.0428       0.0207       0.0954   \n",
       "1         0.0453       0.0523       0.0843       0.0689       0.1183   \n",
       "2         0.0262       0.0582       0.1099       0.1083       0.0974   \n",
       "3         0.0100       0.0171       0.0623       0.0205       0.0205   \n",
       "4         0.0762       0.0666       0.0481       0.0394       0.0590   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "203       0.0187       0.0346       0.0168       0.0177       0.0393   \n",
       "204       0.0323       0.0101       0.0298       0.0564       0.0760   \n",
       "205       0.0522       0.0437       0.0180       0.0292       0.0351   \n",
       "206       0.0303       0.0353       0.0490       0.0608       0.0167   \n",
       "207       0.0260       0.0363       0.0136       0.0272       0.0214   \n",
       "\n",
       "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
       "0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n",
       "1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n",
       "2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n",
       "3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n",
       "4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n",
       "..           ...          ...          ...          ...           ...  ...   \n",
       "203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n",
       "204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n",
       "205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n",
       "206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n",
       "207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n",
       "\n",
       "     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n",
       "0          0.0027        0.0065        0.0159        0.0072        0.0167   \n",
       "1          0.0084        0.0089        0.0048        0.0094        0.0191   \n",
       "2          0.0232        0.0166        0.0095        0.0180        0.0244   \n",
       "3          0.0121        0.0036        0.0150        0.0085        0.0073   \n",
       "4          0.0031        0.0054        0.0105        0.0110        0.0015   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "203        0.0116        0.0098        0.0199        0.0033        0.0101   \n",
       "204        0.0061        0.0093        0.0135        0.0063        0.0063   \n",
       "205        0.0160        0.0029        0.0051        0.0062        0.0089   \n",
       "206        0.0086        0.0046        0.0126        0.0036        0.0035   \n",
       "207        0.0146        0.0129        0.0047        0.0039        0.0061   \n",
       "\n",
       "     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n",
       "0          0.0180        0.0084        0.0090        0.0032   Rock  \n",
       "1          0.0140        0.0049        0.0052        0.0044   Rock  \n",
       "2          0.0316        0.0164        0.0095        0.0078   Rock  \n",
       "3          0.0050        0.0044        0.0040        0.0117   Rock  \n",
       "4          0.0072        0.0048        0.0107        0.0094   Rock  \n",
       "..            ...           ...           ...           ...    ...  \n",
       "203        0.0065        0.0115        0.0193        0.0157   Mine  \n",
       "204        0.0034        0.0032        0.0062        0.0067   Mine  \n",
       "205        0.0140        0.0138        0.0077        0.0031   Mine  \n",
       "206        0.0034        0.0079        0.0036        0.0048   Mine  \n",
       "207        0.0040        0.0036        0.0061        0.0115   Mine  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
