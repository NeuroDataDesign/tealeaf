{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation and Explanation of the Random Forest in Python Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- Random forest (RF) is made of many decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding a Decision Tree\n",
    "- Decision tree: building block of a RF; can think of it as series of yes/no questions asked about data to evetually lead to a predicted class\n",
    "- CART algorithm: decision tree built with splits of nodes that lead to greatest reduction in Gini Impurity (read #2 under \"Visualizing a Decision Tree\")\n",
    "- Separates data points into boxes (nodes) based on their classifications using series of straight lines (essentially a nonlinear model built by constructing many linear boundaries)\n",
    "- Feature: data point\n",
    "    - each internal node representts a test on a feature\n",
    "- Label: decision taken regarding the data point after computing all features\n",
    "- Nonparametric model because number of parameters rows with size of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Decision Tree\n",
    "- All nodes (other than leaf/terminal nodes), have 5 parts to consider:\n",
    "    1. Boolean question: asked about the data based on a value of a feature; based on True/False answer, the node is split and the data point moves down the tree\n",
    "    2. Gini Impurity: probability that a randomly chosen sample in a node would be incorrectly labeled if it was labeled by the distribution of samples in the node\n",
    "    $$ I_G(n) = 1 - \\sum_{i=1}^J \\left(p_i \\right)^2$$\n",
    "        - $n$ = node, $J$ = all classes, $p_i^2$ = fraction of examples in each class\n",
    "        - averaged weighted Gini Impurity of 0.0 constitutes a terminal node; otherwise, data point continues down tree \n",
    "        - decision tree searches through features for the value to split on that results in *greatest reduction* in Gini Impurity\n",
    "        - Weighted Gini Impurity (used for non-root nodes):\n",
    "        $$ I_{second layer}=\\frac{n_{left}}{n_{parent}} * I_{left node} + \\frac{n_{right}}{n_{parent}} * I_{right node} $$\n",
    "        - (CHECK RYAN IMPLEMENTATION FOR HOW TO CALCULATE GINI IMPURITY)\n",
    "    3. Samples: number of observations in node\n",
    "    4. Value: number of samples in each class\n",
    "    5. Class: majority classification for points in node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting (Why Forest is better than One Tree)\n",
    "- Getting Gini Impurity of 0.0 for last layer is nice, but there is a chance of overfitting, as that means the tree made no mistakes on the training data\n",
    "- Objective of a machine learning model is to generalize well to new data it has never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
