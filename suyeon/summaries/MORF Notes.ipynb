{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORF Paper Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold Forests (MORF):\n",
    "- Decision forests (random forests, gradient boost trees) have solidified themselves as powerful learning method in supervised settings\n",
    "- Classification: each forest is collection of decision trees whose individual classifications of a data point are aggregated together using majority vote\n",
    "    - Decision trees are relatively interpretable because they can provide an understanding of which features are most important for correct classification\n",
    "- Traditional decision forests are not able to incorporate known continuity between features to learn new features for structured data (images, time series, etc)\n",
    "- For decision forests to utilize known local structure in data, new features encoding this information must be manually constructed\n",
    "- Manifold forests (proposal): at each node in decision tree, sets of random spatially contiguous features are randomly selected using knowledge of the underlying manifold\n",
    "    - Summing intensities of sampled features yields set of projections which can then be evaluated to partition the observations\n",
    "- Random forest: ensemble of decision trees whose individual classifications of a data point are aggregated together using majority vote\n",
    "    - Each tree consists of split nodes and leaf nodes\n",
    "    - Split nodes: associated with subset of data and splits into two child nodes\n",
    "    - Leaf nodes: created once the partition reaches a stopping criterion\n",
    "        - Typically either falling below an impurity score threshold or a minimum number of observations\n",
    "- A decision tree classifies a new observation by assigning it the class of the partition in to which the observation falls\n",
    "- Forest averages the classifications over all decision trees to make final classification\n",
    "- SPORF is recent modification to random forest that has shown improvement over other versions\n",
    "    - Rather than partitioning the data solely along the coordinate axes (standard basis), SPORF creates partitions along axes specified by sparse vectors\n",
    "- Manifold forest method: in the structured setting, the dictionary of projection vectors is modified to take advantage of the underlying manifold on which the data lies\n",
    "    - At each node in the decision tree, MF samples d atoms, yielding d new features per observation\n",
    "    - Proceeds just like SPORF by optimizing the best split according to the Gini index\n",
    "    - This way, MF seeks to learn low-level features in the structured data, such as edges or corners in images\n",
    "    - Can therefore learn the features that best distinguish a class\n",
    "- Results:\n",
    "    - MF visibly results in a smoother pixel importance, a result most likely from the continuity of neighboring pixels in selected projections\n",
    "    - Since projections in SPORF have no continuity constraint, those that select high importance pixels will also select pixels of low importance by chance\n",
    "        - Relevant problem in low sample size settings\n",
    "    - MF shows little to no importance of these background pixels by virtue of the modified projection distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
