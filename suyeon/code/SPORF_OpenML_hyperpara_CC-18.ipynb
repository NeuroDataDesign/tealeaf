{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenML CC-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import sklearn\n",
    "from rerf.rerfClassifier import rerfClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "import math\n",
    "from math import log\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "benchmark_suite = openml.study.get_suite('OpenML-CC18')  # obtain the benchmark suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization_grid(X, y, *argv):\n",
    "    \"\"\"\n",
    "    Given a classifier and a dictionary of hyperparameters, find optimal hyperparameters using GridSearchCV.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "        Input data, shape (n_samples, n_features)\n",
    "    y : numpy.ndarray\n",
    "        Output data, shape (n_samples, n_outputs)\n",
    "    *argv : list of tuples (classifier, hyperparameters)\n",
    "        List of (classifier, hyperparameters) tuples:\n",
    "        classifier : sklearn-compliant classifier\n",
    "            For example sklearn.ensemble.RandomForestRegressor, rerf.rerfClassifier, etc\n",
    "        hyperparameters : dictionary of hyperparameter ranges\n",
    "            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html.\n",
    "    Returns\n",
    "    -------\n",
    "    clf_best_params : dictionary\n",
    "        Dictionary of best hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    clf_best_params = {}\n",
    "\n",
    "    # Iterate over all (classifier, hyperparameters) pairs\n",
    "    for clf, params in argv:\n",
    "\n",
    "        # Run grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            clf, param_grid=params, cv=10, iid=False\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # Save results\n",
    "        clf_best_params[clf] = grid_search.best_params_\n",
    "\n",
    "    return clf_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimen_CC18 = []\n",
    "best_params = []\n",
    "\n",
    "for task_id in benchmark_suite.tasks[0:5]:  # iterate over all tasks\n",
    "#     try:\n",
    "        f = open(\"SPORF_accuracies_CC-18_hyperpara.txt\",\"a\")\n",
    "        task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "        X_CC18, y_CC18 = task.get_X_and_y()  # get the data\n",
    "        dimen_CC18.append(np.shape(X_CC18))\n",
    "        n_features = np.shape(X_CC18)[1]\n",
    "        n_samples = np.shape(X_CC18)[0]\n",
    "\n",
    "        # build a classifier\n",
    "        clf = rerfClassifier(n_estimators=100)\n",
    "        \n",
    "        #specify max_depth and min_sample_splits ranges\n",
    "        max_depth_array = (np.unique(np.round((np.arange(2,math.log(n_samples),\n",
    "                            (math.log(n_samples)-2)/10))))).astype(int)\n",
    "        max_depth_range = np.append(max_depth_array, None)\n",
    "\n",
    "        min_sample_splits_range = (np.unique(np.round((np.arange(1,math.log(n_samples),\n",
    "                                    (math.log(n_samples)-2)/10))))).astype(int)\n",
    "\n",
    "        # specify parameters and distributions to sample from\n",
    "        param_dist = {\"n_estimators\": np.arange(100,550,25),\n",
    "              \"max_depth\": max_depth_range,\n",
    "              \"min_samples_split\": min_sample_splits_range,\n",
    "              \"feature_combinations\": [1,2,3,4,5], \n",
    "              \"max_features\": [\"auto\", \"sqrt\",\"log2\", None, n_features**2]}\n",
    "\n",
    "        clf_best_params = hyperparameter_optimization_grid(X_CC18, y_CC18, (clf, param_dist))\n",
    "        best_params.append(clf_best_params, axis = 0)\n",
    "        print(task_id)\n",
    "        print('Data set: %s: ' % (task.get_dataset().name))\n",
    "        print(clf_best_params)\n",
    "        print('Time: '+ str(datetime.now() - startTime))\n",
    "        f.write('%i,%s,%s,%f,%f,%f,%f,%f\\n' % (task_id,task.get_dataset().name,str(datetime.now() - startTime),clf_best_params[0],clf_best_params[1],clf_best_params[2],clf_best_params[3],clf_best_params[4]))\n",
    "        f.close()\n",
    "#     except:\n",
    "#         print('Error in OpenML CC-18 dataset ' + str(task_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SPORF with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167124\n",
      "Data set: CIFAR_10; Accuracy: 0.4848\n",
      "Time: 4:38:49.795906\n",
      "167125\n",
      "Data set: Internet-Advertisements; Accuracy: 0.9771\n",
      "Time: 0:08:30.094706\n",
      "167140\n",
      "Data set: dna; Accuracy: 0.9513\n",
      "Time: 0:01:19.666527\n",
      "167141\n",
      "Data set: churn; Accuracy: 0.9380\n",
      "Time: 0:01:06.676688\n"
     ]
    }
   ],
   "source": [
    "# clf = sklearn.pipeline.make_pipeline(sklearn.preprocessing.Imputer(), rerfClassifier())\n",
    "\n",
    "# for task_id in benchmark_suite.tasks[68:]:  # iterate over all tasks\n",
    "#     try:\n",
    "#         f = open(\"SPORF_accuracies_CC-18.txt\",\"a\")\n",
    "#         startTime = datetime.now()\n",
    "#         task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "#         openml.config.apikey = '204cdba18d110fd68ad24b131ea92030'  # set the OpenML Api Key\n",
    "#         run = openml.runs.run_model_on_task(clf, task)  # run the classifier on the task\n",
    "#         score = run.get_metric_fn(sklearn.metrics.accuracy_score)  # print accuracy score\n",
    "#         print(task_id)\n",
    "#         print('Data set: %s; Accuracy: %0.4f' % (task.get_dataset().name,score.mean()))\n",
    "#         print('Time: '+ str(datetime.now() - startTime))\n",
    "#         f.write('%i,%s,%0.4f,%s\\n' % (task_id,task.get_dataset().name,score.mean(),str(datetime.now() - startTime)))\n",
    "#         f.close()\n",
    "#     except:\n",
    "#         print('Error in' + str(task_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tealeaf] *",
   "language": "python",
   "name": "conda-env-tealeaf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
